{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f553b9-0cec-4ed7-bf0c-fb4b144e23c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen\n",
      "  Downloading pyautogen-0.2.21-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting diskcache (from pyautogen)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docker (from pyautogen)\n",
      "  Downloading docker-7.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting flaml (from pyautogen)\n",
      "  Downloading FLAML-2.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.17.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pyautogen) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.3 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pyautogen) (1.14.3)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pyautogen) (2.6.4)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pyautogen) (1.0.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pyautogen) (2.4.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pyautogen) (0.5.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.16.3)\n",
      "Requirement already satisfied: packaging>=14.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from docker->pyautogen) (23.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from docker->pyautogen) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from docker->pyautogen) (2.0.7)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from docker->pyautogen) (305.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from tiktoken->pyautogen) (2023.12.25)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from requests>=2.26.0->docker->pyautogen) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from tqdm>4->openai>=1.3->pyautogen) (0.4.6)\n",
      "Downloading pyautogen-0.2.21-py3-none-any.whl (236 kB)\n",
      "   ---------------------------------------- 0.0/236.6 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 41.0/236.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 71.7/236.6 kB 1.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 122.9/236.6 kB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 174.1/236.6 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 204.8/236.6 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 204.8/236.6 kB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------  235.5/236.6 kB 758.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  235.5/236.6 kB 758.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 236.6/236.6 kB 628.7 kB/s eta 0:00:00\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 41.0/45.5 kB 960.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.5/45.5 kB 751.3 kB/s eta 0:00:00\n",
      "Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.6 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 41.0/147.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 41.0/147.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 41.0/147.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ---------------------- 61.4/147.6 kB 272.3 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 112.6/147.6 kB 409.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 112.6/147.6 kB 409.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 122.9/147.6 kB 327.4 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 122.9/147.6 kB 327.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 147.6/147.6 kB 313.5 kB/s eta 0:00:00\n",
      "Downloading FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
      "   ---------------------------------------- 0.0/296.7 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/296.7 kB 991.0 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 41.0/296.7 kB 991.0 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/296.7 kB 544.7 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/296.7 kB 544.7 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/296.7 kB 544.7 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/296.7 kB 544.7 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/296.7 kB 196.9 kB/s eta 0:00:02\n",
      "   ------------ -------------------------- 92.2/296.7 kB 249.8 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 122.9/296.7 kB 288.1 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 122.9/296.7 kB 288.1 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 143.4/296.7 kB 275.0 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 174.1/296.7 kB 308.5 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 204.8/296.7 kB 327.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 235.5/296.7 kB 360.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 286.7/296.7 kB 411.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 296.7/296.7 kB 417.0 kB/s eta 0:00:00\n",
      "Installing collected packages: flaml, diskcache, docker, pyautogen\n",
      "Successfully installed diskcache-5.6.3 docker-7.0.0 flaml-2.1.2 pyautogen-0.2.21\n"
     ]
    }
   ],
   "source": [
    "!pip install pyautogen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf20574-8c39-4eda-9978-b028d9c3f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\umipa\\anaconda3\\lib\\site-packages (4.19.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9137541-d6fd-431f-b3b5-965fb20d8190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\umipa\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\umipa\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3072122d-3adc-41be-b574-c98e72a2f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUserProxy\u001b[0m (to WebScraper):\n",
      "\n",
      "Can you scrape cndi.run for me? \n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWebScraper\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_Ve7RTwU1P6WjxGvu79FxJ8Ze): scrape_website *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"url\": \"https://cndi.run\"\n",
      "}\n",
      "\u001b[32m*******************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION scrape_website...\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start with a Template for a popular service and CNDI will help you deploy it on your own infrastructure, just as easily as you can sign up for a Platform as a Service.\n",
      "Once your cluster is set up, manage the infrastructure and applications with ease using GitOps and Infrastructure as Code.\n",
      "If you'd like to see a walkthrough for setting up an Airflow cluster using CNDI, checkout this demo:\n",
      "To install CNDI we just need to download the binary and add it to our PATH. This can be done using the script below:\n",
      "If you'd prefer to use Windows we have a one-liner for that too described in this short guide.\n",
      "In either case once the script has finished running, the cndi binary and dependencies are installed to ~/.cndi/bin.\n",
      "CNDI is a tool with which to deploy GitOps enabled Kubernetes application clusters on any platform as quickly and easily as possible. The best way to understand this process is to look at it as a lifecycle.\n",
      "The first step in the lifecycle is to initialize the CNDI project. Because CNDI's mechanics are based on the GitOps workflow, we should initialize a Git repository before we do anything else.\n",
      "The best way to create a repo this as a GitHub user is to install the gh cli, and we'll use this tool a bit later too for managing secrets.\n",
      "Now that we have a Git repository, we can initialize a new CNDI project.\n",
      "The best way to get started if you are new to CNDI is to use the interactive cli, so let's look at that first.\n",
      "This will start an interactive cli that will ask you a series of questions, the first prompt is to select a Template. Templates are a CNDI concept, and they can be thought of as a \"blueprint\" for a cloud-native stack. Once you select a Template, CNDI will ask you some general questions about your project and some template-specific questions. Then it will write out a few files inside your project repo.\n",
      "It's also possible to specify a Template URL to initialize with. A Template URL resolves to a CNDI Template file, this means that you are not limited to only Templates the CNDI team has put together, you can point to any arbitrary template file that follows the Template Schema.\n",
      "These Template URLs can be file:// URLs which have an absolute path to the file locally, or typical remote or https:// URLs over the net.\n",
      "CNDI has generated a few files and folders for us based on our Template file. If you want to learn about what CNDI is really creating, the best file to look at is the cndi_config.yaml file in the root of your repository.\n",
      "We break down all of the generated files later in this document in the outputs section.\n",
      "The next step for our one-time project setup is to make sure that we have all the required environment variables for our project in our .env file that CNDI generated.\n",
      "Some of these values are required for every deployment. For example, you always need to have GIT_USERNAME, GIT_TOKEN and GIT_REPO. Some are only required for a specific provider, like AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, which are only needed for aws deployments. Lastly, some are only required for certain Templates, for example all airflow templates require GIT_SYNC_PASSWORD for accessing repos that hold Airflow DAGs.\n",
      "If you didn't use interactive mode you may have some placeholders in that file to overwrite, and they should be easy to spot. CNDI should also tell you if it is missing expected values.\n",
      "When all of the values have been set in your .env file, we want to use the gh cli again, this time to push our secret environment variables to GitHub.\n",
      "Now we are ready for the next phase of the lifecycle!\n",
      "Now that we have initialized our project, CNDI has given us files that describe our infrastructure resources and files that describe what to run on that infrastructure.\n",
      "CNDI has also created a GitHub Action for us which is responsible for calling cndi run. The run command provided in the cndi binary is responsible for calling terraform apply to deploy our infrastructure. To trigger the process we just need to push our changes to repository:\n",
      "After cndi run has exited successfully you should be able to see new resources spinning up in the deployment target you selected. When the nodes come online in that destination, they will join together to form a Kubernetes cluster.\n",
      "As the nodes join the cluster automatically, they are going to begin sharing workloads. Some workloads come bundled, we will call these CNDI platform services. There are a couple such services, one is sealed-secrets, and another is ArgoCD. Sealed Secrets enables storing Kubernetes Secrets within git securely, and ArgoCD is a GitOps tool which monitors a repo for Kubernetes manifests, and applies them.\n",
      "When ArgoCD comes online, it will begin reading files from the cndi/cluster_manifests directory in the GitHub repo we have been pushing to. Ultimately cndi run is only used within GitHub for infrastructure, and ArgoCD is solely responsible for what to run on the cluster.\n",
      "Your cluster will be online in no time!\n",
      "The next phase of the lifecycle is about making changes to your cluster. These changes can be cluster_manifests oriented, if you are making changes to the software running on your infrastructure, or they can be infrastructure oriented if you are horizontally or vertically scaling your cluster.\n",
      "In either case, the approach is the same. Open your cndi_config.yaml file in your editor and make changes to your applications, cluster_manifests, or infrastructure then run:\n",
      "Upon execution of the command you should see that some of the files cndi generated for us before have been modified or supplemented with new files. So far no changes have been made to our cluster. Just like before we need to push the changes up for them to take effect. This is what GitOps is all about, we don't login to our servers to make changes, we simply modify our config, and git push!\n",
      "With these 3 phases you have everything you need to deploy a data infrastructure cluster using CNDI and evolve it over time!\n",
      "When it comes down time to teardown your cluster, there is only one step, just call:\n",
      "This will delete all of the infrastructure resources that CNDI created for you, and from there you can choose either to delete the repo or keep it around for later.\n",
      "We've got a few walkthroughs you can follow if you'd like, one for each deployment target. The walkthroughs demonstrate how to deploy a production grade Airflow cluster using CNDI's airflow Template.\n",
      "If you are interested in using CNDI, these walkthroughs will be entirely transferrable to other applications beyond Airflow.\n",
      "Let's run through the 3 parts of a cndi_config.yaml file. This one file is the key to understanding CNDI, and it's really pretty simple.\n",
      "The infrastructure section is used to define the infrastructure that will power our cluster. The infrastructure section is broken out into 2 distinct categories. The first category is cndi, and it refers to infrastructure abstractions our team has invented that CNDI exposes for you.\n",
      "Currently CNDI exposes only one abstraction, the nodes interface, and it's a wrapper that simplifies deploying Kubernetes cluster nodes. The CNDI nodes interface wraps the compute resources from every deployment target we support.\n",
      "All nodes entries are nearly identical, the most substantial difference is the kind field which is used to specify the deployment target. These node resources and their supporting infrastructure are ultimately provisioned by Terraform, but we've abstracted a lot of complexity through this interface.\n",
      "Declaring a node is simple, we give it a name, we give it some specs, and we add it to the array!\n",
      "Currently we have support for Cloud Providers aws , azure, and gcp using Microk8s, as well as their managed offerings, eks , aks, and gke. Finally we support running a dev cluster locally using multipass.\n",
      "Just like every other component in CNDI, nodes can be updated in our cndi_config.yaml and we can call cndi ow and push the changes to our git remote to modify the cluster accordingly.\n",
      "Don't forget to push up your changes after running cndi ow so that we can kick off automation for you!\n",
      "The second category within infrastructure is terraform. This is where you can define any Terraform resources you want to be provisioned alongside your cluster.\n",
      "💡 You can also use this section to override any of the default Terraform objects that CNDI deploys.\n",
      "Generally, you should be able to customize CNDI resources through the cndi section instead.\n",
      "But, if you do need to patch a Terraform resource CNDI has created for you, you simply need to match the resource name we have used, and specify the fields you want to update.\n",
      "The next thing we need to configure is the applications that will actually run on the cluster. With CNDIv1 we focused on making it a breeze to deploy Apache Airflow in Kubernetes.\n",
      "Lets see how we accomplish this here in this new and improved CNDI:\n",
      "This is built on top of ArgoCD's Application CRDs and Helm Charts. If you have a Helm Chart, CNDI can deploy it!\n",
      "The third aspect of a cndi_config file is the cluster_manifests object. Any objects here will be used as Kubernetes Manifests and they'll be applied to your cluster through ArgoCD. This gives you full access to all the Kubernetes systems and APIs.\n",
      "If you are new to Kubernetes and are unsure what any of that meant, don't sweat it. CNDI is designed to help with that knowledge gap with templates, and you'll learn along the way too!\n",
      "Pro tip! 🤫\n",
      "If you want to add a new Kubernetes Secret to use inside of your Kubernetes cluster via GitOps, we make this possible by encrypting your secrets with sealed-secrets so they can live in your repo securely and be picked up by ArgoCD automatically. To add a secret to your cluster add the value to your .env file and then add a cluster_manifest entry like the one below. After that just call cndi ow to seal your secret.\n",
      "The example below results in sealing the environment variables \"GIT_USERNAME\" and \"GIT_TOKEN\", into the destination secret key names \"GIT_SYNC_USERNAME\" and \"GIT_SYNC_PASSWORD\" respectively.\n",
      "When cndi init is called there are a few files that it produces:\n",
      "a cndi_config.yaml - autogenerated in interactive mode only, described in the configuration section above\n",
      "a .github/workflows folder, with a GitHub Action inside. The workflow is mostly just wrapping the cndi run command in the CNDI binary executable. As such, if you have a different CI system, you can execute the cndi run command on the binary there instead.\n",
      "a cndi/terraform folder, containing the infrastructure resources cndi has generated for terraform, which cndi will apply automatically every time cndi run is executed.\n",
      "a cndi/cluster_manifests folder, containing Kubernetes manifests that will be installed on your new cluster when it is up and running. This includes manifests like Ingress from the cluster_manifests section of your cndi_config.jsonc.\n",
      "a cndi/cluster_manifests/applications folder, which contains a folder for each application defined in the applications section of your cndi_config.yaml, and a generated ArgoCD Application CRD inside that contains our expertly chosen defaults for that App, and the spefic parameters you've specified yourself in the applications section of your cndi_config.yaml.\n",
      "a .env file which contains all of your environment variables that CNDI relies on, these values must be environment variables that are defined and valid when cndi run is executed.\n",
      "a .gitginore file to ensure secret values never get published as source files to your repo\n",
      "a ./README.md file that explains how you can use and modify these files yourself for the lifetime of the cluster\n",
      "ArgoCD's Web UI is a useful tool for visualizing and debugging your cluster resources. Some of our templates setup Ingress for ArgoCD automatically, if you don't have an Ingress you can still access it by following our port-forwarding doc. Once you can see the login screen for ArgoCD you can login with the username admin and the password we set for you in your .env file under the key ARGOCD_ADMIN_PASSWORD.\n",
      "Setting up DNS for your cluster is a critical step if your cluster will be served online. The solution depends on your \"deployment target\". We have a doc coming soon walking through setting up DNS for AWS and GCP coming soon, but in short you just need to point DNS to the load balancer we provisioned for you. In AWS this means using a CNAME record, or an A record for a cluster running on GCP or Azure.\n",
      "If you are hoping to contribute to this project and want to learn the ropes, you are in the right place. Let's start with setting up your environment:\n",
      "The first step as you might expect is to clone this repo. Take note of where you clone to, it will matter later when we setup some convenience aliases.\n",
      "1. Clone Repo:\n",
      "2. Install Deno:\n",
      "Next let's install deno, though it can be installed with a package manager, I would recommend that you install it without one. Once Deno is installed, make sure you add it to your PATH.\n",
      "3. Setup cndi Alias: Let's setup an alias that allows us to use the deno source code as if it were the regular CLI, without colliding with the released cndi binary\n",
      "We're continuously improving CNDI, but if you have an issue, checkout frequently-asked-questions to get unblocked quickly.\n",
      "If you have any other issues or questions please message Matt or Tamika in the Polyseam Discord Chat.\n",
      "Self-Host Cloud-Native Apps with the Ease of PaaS\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUserProxy\u001b[0m (to WebScraper):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUserProxy\u001b[0m (to WebScraper):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_Ve7RTwU1P6WjxGvu79FxJ8Ze) *****\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start with a Template for a popular service and CNDI will help you deploy it on your own infrastructure, just as easily as you can sign up for a Platform as a Service.\n",
      "Once your cluster is set up, manage the infrastructure and applications with ease using GitOps and Infrastructure as Code.\n",
      "If you'd like to see a walkthrough for setting up an Airflow cluster using CNDI, checkout this demo:\n",
      "To install CNDI we just need to download the binary and add it to our PATH. This can be done using the script below:\n",
      "If you'd prefer to use Windows we have a one-liner for that too described in this short guide.\n",
      "In either case once the script has finished running, the cndi binary and dependencies are installed to ~/.cndi/bin.\n",
      "CNDI is a tool with which to deploy GitOps enabled Kubernetes application clusters on any platform as quickly and easily as possible. The best way to understand this process is to look at it as a lifecycle.\n",
      "The first step in the lifecycle is to initialize the CNDI project. Because CNDI's mechanics are based on the GitOps workflow, we should initialize a Git repository before we do anything else.\n",
      "The best way to create a repo this as a GitHub user is to install the gh cli, and we'll use this tool a bit later too for managing secrets.\n",
      "Now that we have a Git repository, we can initialize a new CNDI project.\n",
      "The best way to get started if you are new to CNDI is to use the interactive cli, so let's look at that first.\n",
      "This will start an interactive cli that will ask you a series of questions, the first prompt is to select a Template. Templates are a CNDI concept, and they can be thought of as a \"blueprint\" for a cloud-native stack. Once you select a Template, CNDI will ask you some general questions about your project and some template-specific questions. Then it will write out a few files inside your project repo.\n",
      "It's also possible to specify a Template URL to initialize with. A Template URL resolves to a CNDI Template file, this means that you are not limited to only Templates the CNDI team has put together, you can point to any arbitrary template file that follows the Template Schema.\n",
      "These Template URLs can be file:// URLs which have an absolute path to the file locally, or typical remote or https:// URLs over the net.\n",
      "CNDI has generated a few files and folders for us based on our Template file. If you want to learn about what CNDI is really creating, the best file to look at is the cndi_config.yaml file in the root of your repository.\n",
      "We break down all of the generated files later in this document in the outputs section.\n",
      "The next step for our one-time project setup is to make sure that we have all the required environment variables for our project in our .env file that CNDI generated.\n",
      "Some of these values are required for every deployment. For example, you always need to have GIT_USERNAME, GIT_TOKEN and GIT_REPO. Some are only required for a specific provider, like AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, which are only needed for aws deployments. Lastly, some are only required for certain Templates, for example all airflow templates require GIT_SYNC_PASSWORD for accessing repos that hold Airflow DAGs.\n",
      "If you didn't use interactive mode you may have some placeholders in that file to overwrite, and they should be easy to spot. CNDI should also tell you if it is missing expected values.\n",
      "When all of the values have been set in your .env file, we want to use the gh cli again, this time to push our secret environment variables to GitHub.\n",
      "Now we are ready for the next phase of the lifecycle!\n",
      "Now that we have initialized our project, CNDI has given us files that describe our infrastructure resources and files that describe what to run on that infrastructure.\n",
      "CNDI has also created a GitHub Action for us which is responsible for calling cndi run. The run command provided in the cndi binary is responsible for calling terraform apply to deploy our infrastructure. To trigger the process we just need to push our changes to repository:\n",
      "After cndi run has exited successfully you should be able to see new resources spinning up in the deployment target you selected. When the nodes come online in that destination, they will join together to form a Kubernetes cluster.\n",
      "As the nodes join the cluster automatically, they are going to begin sharing workloads. Some workloads come bundled, we will call these CNDI platform services. There are a couple such services, one is sealed-secrets, and another is ArgoCD. Sealed Secrets enables storing Kubernetes Secrets within git securely, and ArgoCD is a GitOps tool which monitors a repo for Kubernetes manifests, and applies them.\n",
      "When ArgoCD comes online, it will begin reading files from the cndi/cluster_manifests directory in the GitHub repo we have been pushing to. Ultimately cndi run is only used within GitHub for infrastructure, and ArgoCD is solely responsible for what to run on the cluster.\n",
      "Your cluster will be online in no time!\n",
      "The next phase of the lifecycle is about making changes to your cluster. These changes can be cluster_manifests oriented, if you are making changes to the software running on your infrastructure, or they can be infrastructure oriented if you are horizontally or vertically scaling your cluster.\n",
      "In either case, the approach is the same. Open your cndi_config.yaml file in your editor and make changes to your applications, cluster_manifests, or infrastructure then run:\n",
      "Upon execution of the command you should see that some of the files cndi generated for us before have been modified or supplemented with new files. So far no changes have been made to our cluster. Just like before we need to push the changes up for them to take effect. This is what GitOps is all about, we don't login to our servers to make changes, we simply modify our config, and git push!\n",
      "With these 3 phases you have everything you need to deploy a data infrastructure cluster using CNDI and evolve it over time!\n",
      "When it comes down time to teardown your cluster, there is only one step, just call:\n",
      "This will delete all of the infrastructure resources that CNDI created for you, and from there you can choose either to delete the repo or keep it around for later.\n",
      "We've got a few walkthroughs you can follow if you'd like, one for each deployment target. The walkthroughs demonstrate how to deploy a production grade Airflow cluster using CNDI's airflow Template.\n",
      "If you are interested in using CNDI, these walkthroughs will be entirely transferrable to other applications beyond Airflow.\n",
      "Let's run through the 3 parts of a cndi_config.yaml file. This one file is the key to understanding CNDI, and it's really pretty simple.\n",
      "The infrastructure section is used to define the infrastructure that will power our cluster. The infrastructure section is broken out into 2 distinct categories. The first category is cndi, and it refers to infrastructure abstractions our team has invented that CNDI exposes for you.\n",
      "Currently CNDI exposes only one abstraction, the nodes interface, and it's a wrapper that simplifies deploying Kubernetes cluster nodes. The CNDI nodes interface wraps the compute resources from every deployment target we support.\n",
      "All nodes entries are nearly identical, the most substantial difference is the kind field which is used to specify the deployment target. These node resources and their supporting infrastructure are ultimately provisioned by Terraform, but we've abstracted a lot of complexity through this interface.\n",
      "Declaring a node is simple, we give it a name, we give it some specs, and we add it to the array!\n",
      "Currently we have support for Cloud Providers aws , azure, and gcp using Microk8s, as well as their managed offerings, eks , aks, and gke. Finally we support running a dev cluster locally using multipass.\n",
      "Just like every other component in CNDI, nodes can be updated in our cndi_config.yaml and we can call cndi ow and push the changes to our git remote to modify the cluster accordingly.\n",
      "Don't forget to push up your changes after running cndi ow so that we can kick off automation for you!\n",
      "The second category within infrastructure is terraform. This is where you can define any Terraform resources you want to be provisioned alongside your cluster.\n",
      "💡 You can also use this section to override any of the default Terraform objects that CNDI deploys.\n",
      "Generally, you should be able to customize CNDI resources through the cndi section instead.\n",
      "But, if you do need to patch a Terraform resource CNDI has created for you, you simply need to match the resource name we have used, and specify the fields you want to update.\n",
      "The next thing we need to configure is the applications that will actually run on the cluster. With CNDIv1 we focused on making it a breeze to deploy Apache Airflow in Kubernetes.\n",
      "Lets see how we accomplish this here in this new and improved CNDI:\n",
      "This is built on top of ArgoCD's Application CRDs and Helm Charts. If you have a Helm Chart, CNDI can deploy it!\n",
      "The third aspect of a cndi_config file is the cluster_manifests object. Any objects here will be used as Kubernetes Manifests and they'll be applied to your cluster through ArgoCD. This gives you full access to all the Kubernetes systems and APIs.\n",
      "If you are new to Kubernetes and are unsure what any of that meant, don't sweat it. CNDI is designed to help with that knowledge gap with templates, and you'll learn along the way too!\n",
      "Pro tip! 🤫\n",
      "If you want to add a new Kubernetes Secret to use inside of your Kubernetes cluster via GitOps, we make this possible by encrypting your secrets with sealed-secrets so they can live in your repo securely and be picked up by ArgoCD automatically. To add a secret to your cluster add the value to your .env file and then add a cluster_manifest entry like the one below. After that just call cndi ow to seal your secret.\n",
      "The example below results in sealing the environment variables \"GIT_USERNAME\" and \"GIT_TOKEN\", into the destination secret key names \"GIT_SYNC_USERNAME\" and \"GIT_SYNC_PASSWORD\" respectively.\n",
      "When cndi init is called there are a few files that it produces:\n",
      "a cndi_config.yaml - autogenerated in interactive mode only, described in the configuration section above\n",
      "a .github/workflows folder, with a GitHub Action inside. The workflow is mostly just wrapping the cndi run command in the CNDI binary executable. As such, if you have a different CI system, you can execute the cndi run command on the binary there instead.\n",
      "a cndi/terraform folder, containing the infrastructure resources cndi has generated for terraform, which cndi will apply automatically every time cndi run is executed.\n",
      "a cndi/cluster_manifests folder, containing Kubernetes manifests that will be installed on your new cluster when it is up and running. This includes manifests like Ingress from the cluster_manifests section of your cndi_config.jsonc.\n",
      "a cndi/cluster_manifests/applications folder, which contains a folder for each application defined in the applications section of your cndi_config.yaml, and a generated ArgoCD Application CRD inside that contains our expertly chosen defaults for that App, and the spefic parameters you've specified yourself in the applications section of your cndi_config.yaml.\n",
      "a .env file which contains all of your environment variables that CNDI relies on, these values must be environment variables that are defined and valid when cndi run is executed.\n",
      "a .gitginore file to ensure secret values never get published as source files to your repo\n",
      "a ./README.md file that explains how you can use and modify these files yourself for the lifetime of the cluster\n",
      "ArgoCD's Web UI is a useful tool for visualizing and debugging your cluster resources. Some of our templates setup Ingress for ArgoCD automatically, if you don't have an Ingress you can still access it by following our port-forwarding doc. Once you can see the login screen for ArgoCD you can login with the username admin and the password we set for you in your .env file under the key ARGOCD_ADMIN_PASSWORD.\n",
      "Setting up DNS for your cluster is a critical step if your cluster will be served online. The solution depends on your \"deployment target\". We have a doc coming soon walking through setting up DNS for AWS and GCP coming soon, but in short you just need to point DNS to the load balancer we provisioned for you. In AWS this means using a CNAME record, or an A record for a cluster running on GCP or Azure.\n",
      "If you are hoping to contribute to this project and want to learn the ropes, you are in the right place. Let's start with setting up your environment:\n",
      "The first step as you might expect is to clone this repo. Take note of where you clone to, it will matter later when we setup some convenience aliases.\n",
      "1. Clone Repo:\n",
      "2. Install Deno:\n",
      "Next let's install deno, though it can be installed with a package manager, I would recommend that you install it without one. Once Deno is installed, make sure you add it to your PATH.\n",
      "3. Setup cndi Alias: Let's setup an alias that allows us to use the deno source code as if it were the regular CLI, without colliding with the released cndi binary\n",
      "We're continuously improving CNDI, but if you have an issue, checkout frequently-asked-questions to get unblocked quickly.\n",
      "If you have any other issues or questions please message Matt or Tamika in the Polyseam Discord Chat.\n",
      "Self-Host Cloud-Native Apps with the Ease of PaaS\n",
      "\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWebScraper\u001b[0m (to UserProxy):\n",
      "\n",
      "# Heilmeier Catechism Framework\n",
      "\n",
      "1. **What are you trying to do? Articulate your objectives using absolutely no jargon.**\n",
      "    - CNDI (Cloud-Native Distributed Infrastructure) is working to simplify the deployment of cloud-native applications on any platform. It aims to ease the management of infrastructure and application using GitOps and Infrastructure as Code.\n",
      "\n",
      "2. **How is it done today, and what are the limits of current practice?**\n",
      "    - Today, deploying GitOps enabled Kubernetes application clusters on any platform requires deeper knowledge and hands-on experience. The limit is that it is not straightforward and might be overwhelming for someone new.\n",
      "\n",
      "3. **What is new in your approach and why do you think it will be successful?**\n",
      "    - CNDI is introducing a new approach where they are simplifying the deployment process. With the help of an interactive command-line interface, users can easily set up their project. It uses templates to define the cloud-native stack, which simplifies the process. This approach can be successful as it reduces the complexity and allows users to manage their infrastructure with ease.\n",
      "\n",
      "4. **Who cares? If you are successful, what difference will it make?**\n",
      "    - If successful, it would make a significant difference to developers, IT professionals, and organizations planning to deploy GitOps enabled Kubernetes application clusters. It will simplify their deployment and management processes.\n",
      "\n",
      "5. **What are the risks?**\n",
      "   - The risks could include complexities in setting up the initial environment or adapting to a new command-line interface. Furthermore, as it simplifies a lot of processes, in-depth understanding or troubleshooting might be challenging for users without a thorough understanding of what’s going under the hood.\n",
      "\n",
      "6. **How much will it cost?**\n",
      "   - The cost is not explicitly provided on the website. Users would need to contact the CNDI team for specific pricing details.\n",
      "\n",
      "7. **How long will it take?**\n",
      "   - The time taken will depend on the specific user requirements, infrastructure set up, and the user's familiarity with the system. However, once set up, managing the applications and infrastructure can be done relatively quickly.\n",
      "\n",
      "8. **What are the midterm and final “exams” to check for success?**\n",
      "   - The midterm “exam” could be successfully deploying an application cluster on the user's preferred platform. The final “exam” would be the ease of application and infrastructure management using GitOps and Infrastructure as Code, leading to efficient scaling, operations, and maintenance. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing_extensions import Annotated\n",
    "from autogen import ConversableAgent, register_function\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from dotenv import load_dotenv\n",
    "import pandas\n",
    "\n",
    "load_dotenv()\n",
    "def scrape_website(url: Annotated[str, \"The URL of the web page to scrape\"]) -> Annotated[str, \"Scraped content\"]:\n",
    "    # Initialize the WebDriver (replace 'chromedriver' with the path to your WebDriver binary)\n",
    "    driver = webdriver.Chrome()\n",
    "    text_data = \"\"\n",
    "    try:\n",
    "        # Navigate to the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Extract information\n",
    "        # Example: Extract all text from <p> elements\n",
    "        paragraphs = driver.find_elements(By.TAG_NAME , 'p')\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            text_data += paragraph.text + \"\\n\"\n",
    "    finally:\n",
    "        print(text_data)\n",
    "        # Close the browser\n",
    "        # driver.quit()\n",
    "    return text_data\n",
    "# Example usage\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "config_list = [{\"model\": \"gpt-4\", \"api_key\": openai_api_key}]\n",
    "\n",
    "# scraper_agent = ConversableAgent(\n",
    "#     \"WebScraper\",\n",
    "#     llm_config={\"config_list\": config_list},\n",
    "#     system_message=\"You are a web agent that can read data from google sheets. \"\n",
    "#     \"Returns 'TERMINATE' when the scraping is done.\",\n",
    "# )\n",
    "# # Create user proxy agent.\n",
    "# user_proxy_agent = ConversableAgent(\n",
    "#     \"UserProxy\",\n",
    "#     llm_config=False,  # No LLM for this agent.\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     code_execution_config=False,  # No code execution for this agent.\n",
    "#     is_termination_msg=lambda x: x.get(\"content\", \"\") is not None and \"terminate\" in x[\"content\"].lower(),\n",
    "#     default_auto_reply=\"Please continue if not finished, otherwise return 'TERMINATE'.\",\n",
    "# )\n",
    "# register_function(\n",
    "#     scrape_website,\n",
    "#     caller=scraper_agent,\n",
    "#     executor=user_proxy_agent,\n",
    "#     name=\"scrape_website\",\n",
    "#     description=\"Scrape the google sheet and return the content. Summarise the content based cell address.\",\n",
    "# )\n",
    "\n",
    "\n",
    "# chat_result = user_proxy_agent.initiate_chat(\n",
    "#     scraper_agent,\n",
    "#     message=\"Can you scrape cndi.run for me? \",\n",
    "#     summary_method=\"reflection_with_llm\",\n",
    "#     summary_args={\n",
    "#         \"summary_prompt\": \"Summarise the content based cell address.\"\n",
    "#     },\n",
    "# )    \n",
    "    #create an agent which can take shipment_info as an input and generate an email that needs to be sent to the customer\n",
    "# agent = ConversableAgent(\"Customs Broker\",\n",
    "#     system_message=\"\"\"\n",
    "#     Customs Broker. Your job is to generate emails that can be sent to customers stating the next steps required to clear the shipment item or items.\n",
    "\n",
    "#     The email should contain one or more of the following.\n",
    "#     1. If the item/s cannot be imported beacause of custom rules, please state those rules and politely mention the item cannot be imported.\n",
    "#     2. If the item/s can be imported but needs more info such as Price, bill of sale, cuntry of origin or list of ingredients, please craft the email to list all the \n",
    "#         required info from the customer.\n",
    "#     3. The email should include the customer care number so that the customer cn call.\n",
    "#     \"\"\"  ,                       \n",
    "#     llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "#     code_execution_config=False,  # Turn off code execution, by default it is off.\n",
    "#     function_map=None,  # No registered functions, by default it is None.\n",
    "#     human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "# )\n",
    "\n",
    "# reply = agent.generate_reply(messages=[{\"content\": \"honey\", \"role\": \"user\"}])\n",
    "# print(reply)\n",
    "\n",
    "# create agent that can read a google sheet from the link and forward that data to the customs agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef885cb6-11fe-4990-adde-9be2532c1903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
